{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Breast Cancer by modelling on UCI Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn import tree, linear_model, neighbors\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting OS directory\n",
    "os.chdir('C:\\\\Users\\\\rckar\\\\OneDrive\\\\Documents\\\\MSBA\\\\Fall Semester\\\\6420 Predictive Analytics\\\\HW1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Data\n",
    "df = pd.read_csv(\"wdbc.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming column names\n",
    "col_names = ['id','diagnosis',\n",
    "                 'radius_mean','texture_mean','perimeter_mean','area_mean',\n",
    "                 'smoothness_mean','compactness_mean','concavity_mean',\n",
    "                 'concave points_mean','symmetry_mean','fractal_dimension_mean',\n",
    "                 'radius_se','texture_se','perimeter_se','area_se',\n",
    "                 'smoothness_se','compactness_se','concavity_se','concave points_se',\n",
    "                 'symmetry_se','fractal_dimension_se','radius_worst','texture_worst',\n",
    "                 'perimeter_worst','area_worst','smoothness_worst','compactness_worst',\n",
    "                 'concavity_worst','concave points_worst','symmetry_worst','fractal_dimension_worst']\n",
    "\n",
    "df.columns = col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data pre-processing\n",
    "\n",
    "# checking for null values\n",
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['M', 'B'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>diagnosis_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  diagnosis_class\n",
       "0         M                0\n",
       "1         M                0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factorizing diagnosis as diagnosis class for modelling purpose\n",
    "df['diagnosis_class'],class_names = pd.factorize(df['diagnosis'])\n",
    "print(class_names)\n",
    "df[['diagnosis','diagnosis_class']].head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    357\n",
       "0    212\n",
       "Name: diagnosis_class, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for class imbalance\n",
    "df['diagnosis_class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not observe heavy class imbalace in the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating the Train and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = df.iloc[:,2:32]\n",
    "y_df = df.iloc[:,32:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test_holdout, y_train, y_test_holdout = train_test_split(X_df, y_df, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECISION TREE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tuning the model using GridSearch Cross Validation to find the values of hyperparameters that best fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'max_depth': range(1, 20), 'min_samples_split': range(2, 30), 'criterion': ['gini', 'entropy']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper parameter tuning using GridSearch\n",
    "param_set ={'max_depth': range(1,20), 'min_samples_split' : range(2,30), 'criterion' : [\"gini\", \"entropy\"]}\n",
    "clf_DTree = tree.DecisionTreeClassifier()\n",
    "grid_DTree = GridSearchCV(clf_DTree, param_grid = param_set, cv=10)\n",
    "grid_DTree.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examining the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score achieved across all parameters:  0.9436619718309859\n",
      " \n",
      "Best parameters\n",
      "{'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 4}\n",
      " \n",
      "Best estimator\n",
      "DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=4,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "\n",
    "print(\"Best score achieved across all parameters: \", grid_DTree.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters used to generate that score\n",
    "print(\" \")\n",
    "print(\"Best parameters\")\n",
    "print(grid_DTree.best_params_)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Best estimator\")\n",
    "print(grid_DTree.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross - validation with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the scores for each model run\n",
      "[0.93181818 0.90909091 0.88372093 0.95348837 0.95238095 0.9047619\n",
      " 0.97619048 0.9047619  0.95238095 0.97619048]\n",
      " \n",
      "Mean Accuracy and variance: 0.93 (+/- 0.06)\n"
     ]
    }
   ],
   "source": [
    "clf_Dtree_best = grid_DTree.best_estimator_\n",
    "\n",
    "scores = cross_val_score(clf_Dtree_best, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Below are the scores for each model run\")\n",
    "print(scores)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Mean Accuracy and variance: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the Train data and Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree : accuracy on test data is  95.8 %\n",
      " \n",
      "Decision Tree : Confusion Matrix\n",
      " \n",
      "[[57  1]\n",
      " [ 5 80]]\n",
      " \n",
      "Decision Tree : Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95        58\n",
      "           1       0.99      0.94      0.96        85\n",
      "\n",
      "   micro avg       0.96      0.96      0.96       143\n",
      "   macro avg       0.95      0.96      0.96       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_Dtree_best.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf_Dtree_best.predict(X_test_holdout)\n",
    "print(\"Decision Tree : accuracy on test data is \",round(accuracy_score(y_test_holdout, y_pred)*100,2),\"%\")\n",
    "print(\" \")\n",
    "print(\"Decision Tree : Confusion Matrix\")\n",
    "print(\" \")\n",
    "print(confusion_matrix(y_test_holdout, y_pred))\n",
    "print(\" \")\n",
    "print(\"Decision Tree : Classification Report\")\n",
    "print(classification_report(y_test_holdout, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tuning the model using GridSearch Cross Validation to find the values of hyperparameters that best fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'penalty': ['l1', 'l2'], 'C': array([0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'class_weight': [None, 'balanced']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_set ={'penalty' : [\"l1\",\"l2\"], 'C':np.arange(0.2,1,0.1), 'class_weight': [None, 'balanced']}\n",
    "\n",
    "clf_logistic = linear_model.LogisticRegression()\n",
    "\n",
    "grid_logistic = GridSearchCV(clf_logistic, param_grid = param_set, cv=10)\n",
    "grid_logistic.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### examine the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score achieved across all parameters:  0.9530516431924883\n",
      " \n",
      "Best parameters\n",
      "{'C': 0.5000000000000001, 'class_weight': 'balanced', 'penalty': 'l1'}\n",
      " \n",
      "Best estimator\n",
      "LogisticRegression(C=0.5000000000000001, class_weight='balanced', dual=False,\n",
      "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
      "          multi_class='warn', n_jobs=None, penalty='l1', random_state=None,\n",
      "          solver='warn', tol=0.0001, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Single best score achieved across all params\n",
    "print(\"Best score achieved across all parameters: \", grid_logistic.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters used to generate that score\n",
    "print(\" \")\n",
    "print(\"Best parameters\")\n",
    "print(grid_logistic.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(\" \")\n",
    "print(\"Best estimator\")\n",
    "print(grid_logistic.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross - validation with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the scores for each model run\n",
      "[0.90909091 1.         0.93023256 0.97674419 0.92857143 0.95238095\n",
      " 0.97619048 0.92857143 0.95238095 0.97619048]\n",
      " \n",
      "Mean Accuracy and variance: 0.95 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "clf_logistic_best = grid_logistic.best_estimator_\n",
    "\n",
    "scores = cross_val_score(clf_logistic_best, X_train, y_train, cv=10)\n",
    "\n",
    "print(\"Below are the scores for each model run\")\n",
    "print(scores)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Mean Accuracy and variance: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the Train data and Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : accuracy on test data is  97.2 %\n",
      " \n",
      "Logistic Regression : Confusion Matrix\n",
      " \n",
      "[[57  1]\n",
      " [ 3 82]]\n",
      " \n",
      "Logistic Regression : Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.97        58\n",
      "           1       0.99      0.96      0.98        85\n",
      "\n",
      "   micro avg       0.97      0.97      0.97       143\n",
      "   macro avg       0.97      0.97      0.97       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_logistic_best.fit(X_train,y_train)\n",
    "\n",
    "y_pred = clf_logistic_best.predict(X_test_holdout)\n",
    "\n",
    "print(\"Logistic Regression : accuracy on test data is \",round(accuracy_score(y_test_holdout, y_pred)*100,2),\"%\")\n",
    "print(\" \")\n",
    "print(\"Logistic Regression : Confusion Matrix\")\n",
    "print(\" \")\n",
    "print(confusion_matrix(y_test_holdout, y_pred))\n",
    "print(\" \")\n",
    "print(\"Logistic Regression : Classification Report\")\n",
    "print(classification_report(y_test_holdout, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalizing the input parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = scaler.fit_transform(X_train)\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled)\n",
    "\n",
    "x_test_scaled = scaler.fit_transform(X_test_holdout)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Tuning the model using GridSearch Cross Validation to find the values of hyperparameters that best fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise-deprecating',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29], 'weights': ['uniform', 'distance']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_set ={'n_neighbors': list(range(1,30)), 'weights': [\"uniform\", \"distance\"]}\n",
    "\n",
    "clf_knn = neighbors.KNeighborsClassifier()\n",
    "\n",
    "grid_knn = GridSearchCV(clf_knn, param_grid = param_set, cv=10)\n",
    "grid_knn.fit(x_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### examine the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score achieved across all parameters:  0.9788732394366197\n",
      " \n",
      "Best parameters\n",
      "{'n_neighbors': 10, 'weights': 'uniform'}\n",
      " \n",
      "Best estimator\n",
      "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=None, n_neighbors=10, p=2,\n",
      "           weights='uniform')\n"
     ]
    }
   ],
   "source": [
    "# Single best score achieved across all params\n",
    "print(\"Best score achieved across all parameters: \", grid_knn.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters used to generate that score\n",
    "print(\" \")\n",
    "print(\"Best parameters\")\n",
    "print(grid_knn.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(\" \")\n",
    "print(\"Best estimator\")\n",
    "print(grid_knn.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### cross - validation with the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the scores for each model run\n",
      "[0.97727273 0.95454545 1.         0.97674419 0.95238095 0.95238095\n",
      " 0.97619048 1.         1.         1.        ]\n",
      " \n",
      "Mean Accuracy and variance: 0.98 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "clf_knn_best = grid_knn.best_estimator_\n",
    "\n",
    "scores = cross_val_score(clf_knn_best, x_train_scaled, y_train, cv=10)\n",
    "\n",
    "print(\"Below are the scores for each model run\")\n",
    "print(scores)\n",
    "\n",
    "print(\" \")\n",
    "print(\"Mean Accuracy and variance: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting the Train data and Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN : accuracy on test data is  93.71 %\n",
      " \n",
      "KNN : Confusion Matrix\n",
      " \n",
      "[[52  6]\n",
      " [ 3 82]]\n",
      " \n",
      "KNN : Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.90      0.92        58\n",
      "           1       0.93      0.96      0.95        85\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       143\n",
      "   macro avg       0.94      0.93      0.93       143\n",
      "weighted avg       0.94      0.94      0.94       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf_knn_best.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "y_pred = clf_knn_best.predict(X_test_holdout)\n",
    "\n",
    "print(\"KNN : accuracy on test data is \",round(accuracy_score(y_test_holdout, y_pred)*100,2),\"%\")\n",
    "print(\" \")\n",
    "print(\"KNN : Confusion Matrix\")\n",
    "print(\" \")\n",
    "print(confusion_matrix(y_test_holdout, y_pred))\n",
    "print(\" \")\n",
    "print(\"KNN : Classification Report\")\n",
    "print(classification_report(y_test_holdout, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
