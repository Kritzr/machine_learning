{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kritzr/machine_learning/blob/main/nlp_2022503035/nltk_intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb2432fd-43d9-4129-b836-a6787c8e058d",
      "metadata": {
        "tags": [],
        "id": "bb2432fd-43d9-4129-b836-a6787c8e058d",
        "outputId": "03701311-f8fe-4cab-cb69-4e11b1d62da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.1)\n",
            "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
            "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81f1a93-6c38-4b5d-81dc-beec64d3c428",
      "metadata": {
        "tags": [],
        "id": "e81f1a93-6c38-4b5d-81dc-beec64d3c428",
        "outputId": "3e73ef1b-7dc0-4983-f492-3eaa987e5cc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaac29b-53b2-4f85-94c4-8b801a142b6e",
      "metadata": {
        "tags": [],
        "id": "adaac29b-53b2-4f85-94c4-8b801a142b6e",
        "outputId": "278bdc8b-7069-47f4-b645-05bce1c5191d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['My', 'name', 'is', 'Krithika', 'Ravishankar/People', 'call', 'me', 'gangsta', '.']\n",
            "['My name is Krithika Ravishankar/People call me gangsta.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "sent = \"My name is Krithika Ravishankar/People call me gangsta.\"\n",
        "print(word_tokenize(sent))\n",
        "print(sent_tokenize(sent))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73c6b7df-de95-4bb3-91e7-7b8cf6263297",
      "metadata": {
        "tags": [],
        "id": "73c6b7df-de95-4bb3-91e7-7b8cf6263297",
        "outputId": "547f6c5d-ec96-484b-cc4d-dcffcb3c878b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['get', 'this', 'do', 'you', 'know', 'who', 'i', 'am', 'My', 'name', 'is', 'Krithika', 'Ravishankar', 'People', 'call', 'me', 'gangsta', 'i', 'am', '73rd', 'gangsta', 'of', 'my', 'generation']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "text = \"get this: do you know who i am ? My name is Krithika Ravishankar , People call me gangsta! i am 73rd gangsta of my generation\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a14c89-df5f-4575-96c6-cd1e7aa01623",
      "metadata": {
        "tags": [],
        "id": "15a14c89-df5f-4575-96c6-cd1e7aa01623",
        "outputId": "d3d822a8-2364-4b97-f9ba-094ce7d291c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "danc\n",
            "dancer\n",
            "danc\n",
            "danc\n",
            "danc\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter= PorterStemmer()\n",
        "\n",
        "\n",
        "print(porter.stem(\"dance\"))\n",
        "print(porter.stem(\"dancer\"))\n",
        "print(porter.stem(\"dances\"))\n",
        "print(porter.stem(\"dancing\"))\n",
        "print(porter.stem(\"danced\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4296148-4da3-4a97-96d4-c9d29cf9596e",
      "metadata": {
        "tags": [],
        "id": "f4296148-4da3-4a97-96d4-c9d29cf9596e",
        "outputId": "efa4fa79-b0e0-4a7e-b519-7958032e25f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dance\n",
            "dance\n",
            "dance\n",
            "dance\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"dance\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"danced\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"dancing\", 'v'))\n",
        "print(lemmatizer.lemmatize(\"dances\", 'v'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "458885c0-a24e-4981-8624-6c3deaa92ddd",
      "metadata": {
        "tags": [],
        "id": "458885c0-a24e-4981-8624-6c3deaa92ddd",
        "outputId": "7ac22f91-1a46-4062-857a-23b8393abc51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Alice', 'NNP'),\n",
              " ('quickly', 'RB'),\n",
              " ('ran', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('Computer', 'NNP'),\n",
              " ('Science', 'NNP'),\n",
              " ('building', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('London', 'NNP'),\n",
              " (',', ','),\n",
              " ('where', 'WRB'),\n",
              " ('she', 'PRP'),\n",
              " ('met', 'VBD'),\n",
              " ('Bob', 'NNP'),\n",
              " (',', ','),\n",
              " ('a', 'DT'),\n",
              " ('software', 'NN'),\n",
              " ('engineer', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "from nltk import word_tokenize\n",
        "\n",
        "text = \"Alice quickly ran to the Computer Science building in London, where she met Bob, a software engineer.\"\n",
        "tokenized_text = word_tokenize(text)\n",
        "tags = pos_tag(tokenized_text)\n",
        "tags\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c31b7821-e6a0-42a6-9697-97e3c09ecce4",
      "metadata": {
        "tags": [],
        "id": "c31b7821-e6a0-42a6-9697-97e3c09ecce4",
        "outputId": "cd64f295-431e-41af-8df3-43fb12c52776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\2022503035\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Alice', 'quickly', 'ran', 'Computer', 'Science', 'building', 'London', ',', 'met', 'Bob', ',', 'software', 'engineer', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in tokenized_text if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1700d167-e744-42b4-9258-76435f620782",
      "metadata": {
        "tags": [],
        "id": "1700d167-e744-42b4-9258-76435f620782"
      },
      "outputs": [],
      "source": [
        "lower_text = [word.lower() for word in tokenized_text]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8b4147da-7d35-4d41-811e-bd52b8391fe6",
      "metadata": {
        "id": "8b4147da-7d35-4d41-811e-bd52b8391fe6",
        "outputId": "33fa3c6d-1525-4a15-f8a3-8fa78170dc4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy\n",
            "  Downloading numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m493.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        }
      ],
      "source": [
        "pip install --force-reinstall numpy gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.21.5\n",
        "!pip install gensim==4.3.3\n",
        "!pip install thinc==8.0.0"
      ],
      "metadata": {
        "id": "Yv82KJOW5812",
        "outputId": "707ae8d9-8ab6-42f4-e0c1-723944e9eccf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Yv82KJOW5812",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement numpy==1.21.5 (from versions: 1.3.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.6.1, 1.6.2, 1.7.0, 1.7.1, 1.7.2, 1.8.0, 1.8.1, 1.8.2, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.10.0.post2, 1.10.1, 1.10.2, 1.10.4, 1.11.0, 1.11.1, 1.11.2, 1.11.3, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 1.13.3, 1.14.0, 1.14.1, 1.14.2, 1.14.3, 1.14.4, 1.14.5, 1.14.6, 1.15.0, 1.15.1, 1.15.2, 1.15.3, 1.15.4, 1.16.0, 1.16.1, 1.16.2, 1.16.3, 1.16.4, 1.16.5, 1.16.6, 1.17.0, 1.17.1, 1.17.2, 1.17.3, 1.17.4, 1.17.5, 1.18.0, 1.18.1, 1.18.2, 1.18.3, 1.18.4, 1.18.5, 1.19.0, 1.19.1, 1.19.2, 1.19.3, 1.19.4, 1.19.5, 1.20.0, 1.20.1, 1.20.2, 1.20.3, 1.21.0, 1.21.1, 1.22.0, 1.22.1, 1.22.2, 1.22.3, 1.22.4, 1.23.0, 1.23.1, 1.23.2, 1.23.3, 1.23.4, 1.23.5, 1.24.0, 1.24.1, 1.24.2, 1.24.3, 1.24.4, 1.25.0, 1.25.1, 1.25.2, 1.26.0, 1.26.1, 1.26.2, 1.26.3, 1.26.4, 2.0.0, 2.0.1, 2.0.2, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.2.0rc1, 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for numpy==1.21.5\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: gensim==4.3.3 in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.3) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.3) (1.17.2)\n",
            "Collecting thinc==8.0.0\n",
            "  Downloading thinc-8.0.0.tar.gz (619 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.4/619.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting blis<0.8.0,>=0.4.0 (from thinc==8.0.0)\n",
            "  Using cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (3.0.9)\n",
            "Collecting wasabi<1.1.0,>=0.4.0 (from thinc==8.0.0)\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl.metadata (28 kB)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (2.5.1)\n",
            "Requirement already satisfied: catalogue<3.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (2.0.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (75.2.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from thinc==8.0.0) (1.26.4)\n",
            "Collecting pydantic<1.8.0,>=1.7.1 (from thinc==8.0.0)\n",
            "  Downloading pydantic-1.7.4-py3-none-any.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "Downloading pydantic-1.7.4-py3-none-any.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.9/107.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Building wheels for collected packages: thinc\n",
            "  Building wheel for thinc (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thinc: filename=thinc-8.0.0-cp311-cp311-linux_x86_64.whl size=3234903 sha256=2dad0867927ff233d04707ca6db253fa592e0c6154fc6ff753d3f8cb17b93d38\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/9f/a1/184d4ca95ee9053fb30148469cee071d853348dbdff28b9b2f\n",
            "Successfully built thinc\n",
            "Installing collected packages: wasabi, pydantic, blis, thinc\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.3.0\n",
            "    Uninstalling blis-1.3.0:\n",
            "      Successfully uninstalled blis-1.3.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.6\n",
            "    Uninstalling thinc-8.3.6:\n",
            "      Successfully uninstalled thinc-8.3.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "spacy 3.8.5 requires thinc<8.4.0,>=8.3.4, but you have thinc 8.0.0 which is incompatible.\n",
            "google-genai 1.12.1 requires pydantic<3.0.0,>=2.0.0, but you have pydantic 1.7.4 which is incompatible.\n",
            "albumentations 2.0.5 requires pydantic>=2.9.2, but you have pydantic 1.7.4 which is incompatible.\n",
            "langchain 0.3.24 requires pydantic<3.0.0,>=2.7.4, but you have pydantic 1.7.4 which is incompatible.\n",
            "openai 1.76.0 requires pydantic<3,>=1.9.0, but you have pydantic 1.7.4 which is incompatible.\n",
            "langchain-core 0.3.56 requires pydantic<3.0.0,>=2.5.2; python_full_version < \"3.12.4\", but you have pydantic 1.7.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed blis-0.7.11 pydantic-1.7.4 thinc-8.0.0 wasabi-0.10.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "JbxeLnHL6LD8",
        "outputId": "32047b70-735d-40c2-904c-f244932d3b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JbxeLnHL6LD8",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize, TreebankWordTokenizer, RegexpTokenizer, WhitespaceTokenizer, TweetTokenizer, PunktSentenceTokenizer, sent_tokenize\n",
        "from nltk.corpus import gutenberg\n",
        "\n",
        "\n",
        "\n",
        "class NLPTokenizer:\n",
        "    def __init__(self, text, tokenizer_type,isLower=True):\n",
        "        self.text = text\n",
        "        self.tokenizer_type = tokenizer_type\n",
        "        self.isLower = isLower\n",
        "    def process(self):\n",
        "      if self.isLower:\n",
        "        self.text=self.text.lower()\n",
        "    def tokenizer(self):\n",
        "        self.process()\n",
        "        if self.tokenizer_type == \"word_tokenize\":\n",
        "            return word_tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"treebank\":\n",
        "            tokenizer = TreebankWordTokenizer()\n",
        "            return tokenizer.tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"regexp\":\n",
        "            tokenizer = RegexpTokenizer(r'\\w+')\n",
        "            return tokenizer.tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"whitespace\":\n",
        "            tokenizer = WhitespaceTokenizer()\n",
        "            return tokenizer.tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"tweet\":\n",
        "            tokenizer = TweetTokenizer()\n",
        "            return tokenizer.tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"sent_tokenize\":\n",
        "            return sent_tokenize(self.text)\n",
        "\n",
        "        elif self.tokenizer_type == \"punkt\":\n",
        "            tokenizer = PunktSentenceTokenizer()\n",
        "            return tokenizer.tokenize(self.text)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown tokenizer type: {self.tokenizer_type}\")\n",
        "\n",
        "    def print_tokens(self):\n",
        "        tokens = self.tokenizer()\n",
        "        print(f\"\\n--- Tokens using {self.tokenizer_type} ---\")\n",
        "        print(tokens)\n",
        "        print(\"-\"*200)\n",
        "\n",
        "\n",
        "sample_text = gutenberg.raw('austen-emma.txt')[:300]\n",
        "\n",
        "\n",
        "tokenizer_types = [\"word_tokenize\", \"treebank\", \"regexp\", \"whitespace\", \"tweet\", \"sent_tokenize\", \"punkt\"]\n",
        "print(\"-\"*200)\n",
        "print(\"With lower casing \")\n",
        "print(\"-\"*200)\n",
        "for t_type in tokenizer_types:\n",
        "    tokenizer = NLPTokenizer(sample_text, t_type)\n",
        "    tokenizer.print_tokens()\n",
        "print(\"-\"*200)\n",
        "print(\"Without lower casing\")\n",
        "print(\"-\"*200)\n",
        "for t_type in tokenizer_types:\n",
        "    tokenizer = NLPTokenizer(sample_text, t_type,False)\n",
        "    tokenizer.print_tokens()\n"
      ],
      "metadata": {
        "id": "MYIAub1S6UpL",
        "outputId": "5ce5973a-d4ed-46b1-ec18-65baf58549c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MYIAub1S6UpL",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "With lower casing \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using word_tokenize ---\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using treebank ---\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'she', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using regexp ---\n",
            "['emma', 'by', 'jane', 'austen', '1816', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'she', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using whitespace ---\n",
            "['[emma', 'by', 'jane', 'austen', '1816]', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'she', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using tweet ---\n",
            "['[', 'emma', 'by', 'jane', 'austen', '1816', ']', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'she', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using sent_tokenize ---\n",
            "['[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', 'she was t']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using punkt ---\n",
            "['[emma by jane austen 1816]\\n\\nvolume i\\n\\nchapter i\\n\\n\\nemma woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', 'she was t']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Without lower casing\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using word_tokenize ---\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using treebank ---\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'She', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using regexp ---\n",
            "['Emma', 'by', 'Jane', 'Austen', '1816', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', 'and', 'had', 'lived', 'nearly', 'twenty', 'one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', 'She', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using whitespace ---\n",
            "['[Emma', 'by', 'Jane', 'Austen', '1816]', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence;', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her.', 'She', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using tweet ---\n",
            "['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition', ',', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best', 'blessings', 'of', 'existence', ';', 'and', 'had', 'lived', 'nearly', 'twenty-one', 'years', 'in', 'the', 'world', 'with', 'very', 'little', 'to', 'distress', 'or', 'vex', 'her', '.', 'She', 'was', 't']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using sent_tokenize ---\n",
            "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', 'She was t']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "--- Tokens using punkt ---\n",
            "['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', 'She was t']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "class StopwordRemover:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def remove_stopwords(self):\n",
        "    tokenizer=NLPTokenizer(self.text,'word_tokenize',False)\n",
        "    tokens=tokenizer.tokenizer()\n",
        "    stop_words=set(stopwords.words('english'))\n",
        "    filtered_tokens=[word for word in tokens if word.lower() not in stop_words]\n",
        "    return filtered_tokens\n",
        "  def print_output(self):\n",
        "    output=self.remove_stopwords()\n",
        "    print(f\"\\n--- Tokens after removing stopwords ---\")\n",
        "    print(output)\n",
        "    print(\"-\"*200)\n",
        "\n",
        "sample_text = \"Emma, by Jane Austen (1816): Emma Woodhouse, handsome, clever, and rich, had a happy disposition.\"\n",
        "\n",
        "\n",
        "stopword_remover = StopwordRemover(sample_text)\n",
        "stopword_remover.print_output()\n"
      ],
      "metadata": {
        "id": "2gbzvcJ66WAY",
        "outputId": "196f4bc0-47c7-4310-bfa4-0968d56241ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2gbzvcJ66WAY",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokens after removing stopwords ---\n",
            "['Emma', ',', 'Jane', 'Austen', '(', '1816', ')', ':', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'rich', ',', 'happy', 'disposition', '.']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Stemmer:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def stem_words(self):\n",
        "    tokenizer=NLPTokenizer(self.text,'word_tokenize',False)\n",
        "    tokens=tokenizer.tokenizer()\n",
        "    stemmer=PorterStemmer()\n",
        "    stemmed_tokens=[stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "  def print_output(self):\n",
        "    output=self.stem_words()\n",
        "    print(f\"\\n--- Tokens after stemming ---\")\n",
        "    print(output)\n",
        "    print(\"-\"*200)\n",
        "\n",
        "sample_text = \"Emma, by Jane Austen (1816): Emma Woodhouse, handsome, clever, and rich, had a happy disposition.\"\n",
        "\n",
        "\n",
        "stemmer = Stemmer(sample_text)\n",
        "stemmer.print_output()"
      ],
      "metadata": {
        "id": "hdbSAF1J6ZlU",
        "outputId": "2a40296a-f7fc-482e-a23e-1d23b406f2bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hdbSAF1J6ZlU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokens after stemming ---\n",
            "['emma', ',', 'by', 'jane', 'austen', '(', '1816', ')', ':', 'emma', 'woodhous', ',', 'handsom', ',', 'clever', ',', 'and', 'rich', ',', 'had', 'a', 'happi', 'disposit', '.']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lemmatizer:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "  def lemmatize_words(self):\n",
        "    tokenizer=NLPTokenizer(self.text,'word_tokenize',False)\n",
        "    tokens=tokenizer.tokenizer()\n",
        "    lemmatizer=WordNetLemmatizer()\n",
        "    lemmatized_tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return lemmatized_tokens\n",
        "  def print_output(self):\n",
        "    output=self.lemmatize_words()\n",
        "    print(f\"\\n--- Tokens after lemmatization ---\")\n",
        "    print(output)\n",
        "    print(\"-\"*200)\n",
        "\n",
        "sample_text = \"Emma, by Jane Austen (1816): Emma Woodhouse, handsome, clever, and rich, had a happy disposition.\"\n",
        "lemmatizer = Lemmatizer(sample_text)\n",
        "lemmatizer.print_output()"
      ],
      "metadata": {
        "id": "JiCyfqeu6dd7",
        "outputId": "bc482be1-28c1-4452-c2ee-f0cbe4344d5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "JiCyfqeu6dd7",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Tokens after lemmatization ---\n",
            "['Emma', ',', 'by', 'Jane', 'Austen', '(', '1816', ')', ':', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'had', 'a', 'happy', 'disposition', '.']\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PunctuationRemover:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def remove_punctation(self):\n",
        "    translator=str.maketrans('','',string.punctuation)\n",
        "    no_punct_text=self.text.translate(translator)\n",
        "    return no_punct_text\n",
        "  def print_output(self):\n",
        "    output=self.remove_punctation()\n",
        "    print(f\"\\n--- Text after removing punctuation ---\")\n",
        "    print(output)\n",
        "    print(\"-\"*200)\n",
        "sample_text = \"Emma, by Jane Austen (1816): Emma Woodhouse, handsome, clever, and rich, had a happy disposition.\"\n",
        "punctuation_remover = PunctuationRemover(sample_text)\n",
        "punctuation_remover.print_output()"
      ],
      "metadata": {
        "id": "zSgV6Cjv6g60",
        "outputId": "a71a9e84-8ede-4543-ec22-e4df3eefb861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zSgV6Cjv6g60",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Text after removing punctuation ---\n",
            "Emma by Jane Austen 1816 Emma Woodhouse handsome clever and rich had a happy disposition\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tJ8bk-Yg6jdO"
      },
      "id": "tJ8bk-Yg6jdO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}